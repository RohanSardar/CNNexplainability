# ğŸ§  CNN Explainability

This repository contains implementations of popular **CNN explainability techniques** such as CAM, Grad-CAM, Grad-CAM++, and more. The goal is to make it easy to visualize what parts of an image a convolutional neural network focuses on when making predictions.

At the moment, **only Class Activation Mapping (CAM)** is implemented.
More methods â€” including **Grad-CAM**, **Grad-CAM++**, **Score-CAM**, and others â€” will be added soon.

## ğŸš§ Project Status

This project is **a work in progress**.
Expect frequent updates as new explainability techniques are added.

## ğŸ–¼ï¸ Class Activation Mapping (CAM)

CAM highlights the image regions that contribute most to a modelâ€™s prediction.
Explore `class_activation_mapping.ipynb`

## ğŸ¤ Contributing

Contributions, suggestions, and ideas are welcome!
Feel free to open an issue or submit a pull request.
